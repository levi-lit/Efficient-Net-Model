# -*- coding: utf-8 -*-
"""face-expression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Scvvt-LJyueq1LoIHz0oxqNgTjxQHoHG

### loading necessary libraries
"""

import tensorflow as tf
import os
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random
import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

from zipfile import ZipFile

with ZipFile('/content/drive/MyDrive/FaceExpressions.zip', 'r') as zip_ref:
        zip_ref.extractall('')

!ls

for dirpath, dirnames, filenames in os.walk("/content/dataset"):
 print(f"There are {len(filenames)} images in '{dirpath}'.")

"""## visualizing the images"""

def view_random_images(target_class, num_images=8):
  target_dir="/content/dataset"
  target_folder = os.path.join(target_dir, target_class)

  random_images = random.sample(os.listdir(target_folder), num_images)

  # Create a subplot to display multiple images
  plt.figure(figsize=(12, 6))
  for i, image in enumerate(random_images):
    plt.subplot(2, 4, i + 1)  # 2 rows, 4 columns
    img = mpimg.imread(os.path.join(target_folder, image))
    plt.imshow(img)
    plt.title(f"{target_class} - {i + 1}")
    plt.axis("off")

  plt.show()

view_random_images("Happy",8)

view_random_images("Sad",8)

view_random_images("Neutral",8)

view_random_images("Angry",8)

view_random_images("Surprise",8)

"""### Data preprocessing"""

data_dir='/content/dataset'
IMG_SIZE = (224, 224) # define image size
total_data= tf.keras.preprocessing.image_dataset_from_directory(directory=data_dir,
                                                                            image_size=IMG_SIZE,
                                                                            label_mode="categorical",
                                                                            batch_size=32)

total_samples = total_data.cardinality().numpy()

train_size = int(0.8 * total_samples)
test_size = total_samples - train_size

# Split the dataset
train_data = total_data.take(train_size)
test_data = total_data.skip(train_size)

print("Number of samples in training set:", train_size)
print("Number of samples in testing set:", test_size)

total_data.class_names

"""### Creating Callbacks
* Tensorbord callback

"""

checkpoint_path="Expresion/checkpoint.ckpt"
checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                          save_weights_only=True,
                                                          save_best_only=True,
                                                          save_freq="epoch",
                                                          verbose=1)

import datetime
def create_tensorboard_callback(dir_name, experiment_name):
  """
  Creatin a TensorBoard callback instand to store log files.

  Stores log files with the filepath:
    "dir_name/experiment_name/current_datetime/"

  Args:
    dir_name: target directory to store TensorBoard log files
    experiment_name: name of experiment directory (e.g. efficientnet_model_1)
  """
  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir
  )
  print(f"Saving TensorBoard log files to: {log_dir}")
  return tensorboard_callback

"""# Mixed_precision
* speedup the training upto 3x
* one change we have to do is that in output layer we have to add dtype='float32'
"""

from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')

"""### Data augmentaion sequencial layer"""

from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
data_augmentation = Sequential([
  layers.RandomFlip("horizontal"),
  layers.RandomRotation(0.2),
  layers.RandomZoom(0.2),
  layers.RandomHeight(0.2),
  layers.RandomWidth(0.2),
  # preprocessing.Rescaling(1./255) # remove for EfficientNetB0
], name ="data_augmentation")

"""### First Model"""

from tensorflow import keras
from tensorflow.keras import layers

# Setup input shape and base model, freezing the base model layers
input_shape = (224, 224, 3)
base_model = tf.keras.applications.EfficientNetB7(include_top=False)
base_model.trainable = False

# Create input layer
inputs = layers.Input(shape=input_shape, name="input_layer")

# Add in data augmentation Sequential model as a layer
#x = data_augmentation(inputs)

# Give base_model the inputs (after augmentation) and don't train it
x = base_model(inputs, training=False)

# Pool output features of the base model
x = layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)

# Put a dense layer on as the output
outputs = layers.Dense(6, activation="softmax",dtype='float32', name="output_layer")(x)
#dtype='float32' because we are using mixpresion

# Make a model using the inputs and outputs
model_1 = keras.Model(inputs, outputs)

from tensorflow.keras.utils import plot_model
plot_model(model_1)

"""I tried but Augmentation Layer do not increse accuracy despite it decrease in my case,so i removed it"""

#complile the model
model_1.compile(loss=tf.keras.losses.categorical_crossentropy,
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=['accuracy'])
model_1.summary()

history_model_1_efficientB7 = model_1.fit(train_data,
                                           steps_per_epoch=len(train_data),
                                           epochs=5,
                                           batch_size=32,
                                           validation_data=test_data,
                                           validation_steps=len(test_data),
                                           callbacks=[checkpoint_callback]
                                       )

# Plot the validation and training data separately
def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  """
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();

plot_loss_curves(history_model_1_efficientB7)

from sklearn.metrics import roc_curve, auc
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
import numpy as np

# Assuming your model is named 'model_1' and your test dataset is 'test_data'
# First, get the true labels and predictions
y_true = np.concatenate([y for x, y in test_data], axis=0)
y_pred = model_1.predict(test_data)

# Since your labels are one-hot encoded, you need to find the indices for the true labels
y_true = np.argmax(y_true, axis=1)

# Calculate ROC curve and ROC area for each class
n_classes = 6 # Number of classes
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(to_categorical(y_true, num_classes=n_classes)[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot of a ROC curve for a specific class
for i in range(n_classes):
    plt.figure()
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()

"""# Lets fine tune our model by unfreezing some of the layer"""

model_1.load_weights(checkpoint_path)

model_1.evaluate(test_data)

# Are these layers trainable?
for layer in model_1.layers:
  print(layer, layer.trainable)

# Access the base_model layers of model_1
model_1_base_model = model_1.layers[1]
model_1_base_model.name

# Make all the layers in model_1_base_model trainable
model_1_base_model.trainable = True

# Freeze all layers except for the last 15
for layer in model_1_base_model.layers[:-15]:
  layer.trainable = False

# Recompile (we have to recompile our models every time we make a change)
model_1.compile(loss="categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # when fine-tuning you typically want to lower the learning rate by 10x*
                metrics=["accuracy"])
print(len(model_1.trainable_variables))

# Fine-tune for another 10 epochs
initial_epoch=5
fine_tune_epochs = initial_epoch + 5

# Refit the model (same as model_1 except with more trainable layers)
history_model_1_efficientB7_fineTune = model_1.fit(train_data,
                                                steps_per_epoch=len(train_data),
                                                epochs=fine_tune_epochs,
                                                batch_size=32,
                                                validation_data=test_data,
                                                validation_steps=len(test_data),
                                                initial_epoch=history_model_1_efficientB7.epoch[-1],# start from the previous last epoch
                                                callbacks=[checkpoint_callback]
                                                           )

model_1.load_weights(checkpoint_path)

model_1.evaluate(test_data)

"""### fine tuning"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np

# Assuming y_true contains the true labels and y_pred contains the predicted probabilities
y_pred_labels = np.argmax(y_pred, axis=1)  # Convert probabilities to predicted labels

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=total_data.class_names)

# Plot the confusion matrix
disp.plot(cmap=plt.cm.Blues)
plt.show()

from sklearn.metrics import roc_auc_score

# Calculate the ROC-AUC score
roc_auc = roc_auc_score(to_categorical(y_true, num_classes=n_classes), y_pred, multi_class='ovr')

print(f"Average ROC-AUC score: {roc_auc:.2f}")

checkpoint_path="BigModel/checkpoint.ckpt"
checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                          save_weights_only=True,
                                                          save_best_only=True,
                                                          save_freq="epoch",
                                                          verbose=1)

base_model=tf.keras.applications.efficientnet_v2.EfficientNetV2L(include_top=False)

base_model.trainable=False

input=tf.keras.layers.Input(shape=(224,224,3),name="inpute_layer")
x=base_model(input)
x=tf.keras.layers.GlobalAveragePooling2D(name="global_avrage_pooling")(x)
outputs=tf.keras.layers.Dense(6,activation="softmax",dtype='float32',name="Output_layer")(x)
model_2=tf.keras.Model(input,outputs)

#complile the model
model_2.compile(loss=tf.keras.losses.categorical_crossentropy,
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=['accuracy'])
model_2.summary()

history_model_2= model_2.fit(train_data,
                            steps_per_epoch=len(train_data),
                            epochs=10,
                            batch_size=32,
                            validation_data=test_data,
                            validation_steps=len(test_data),
                            callbacks=[checkpoint_callback]
                              )

plot_loss_curves(history_model_2)

model_2.load_weights(checkpoint_path)

model_save_path = 'efficient-2.h5'
model_2.save(model_save_path)

from tensorflow.keras.models import load_model

# Load the model
model_loaded2 = load_model('efficient-2.h5')

from tensorflow.keras.models import load_model

# Load the model
model_loaded = load_model('/content/efficient-2.h5')

"""making predictions"""

model_2.save("model_2_trained")

import os
import random
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.efficientnet import preprocess_input
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model

# Base path to your dataset
base_path = '/content/Expresion'

# Class names (folders) to match your dataset structure
class_names = ['Ahegao', 'Angry',  'Happy', 'Neutral','Sad', 'Surprise']

# Function to prepare an image
def prepare_image(img_path, img_height=224, img_width=224):
    """Prepares an image for classification by the model."""
    img = image.load_img(img_path, target_size=(img_height, img_width))
    img_array = image.img_to_array(img)
    img_array_expanded_dims = np.expand_dims(img_array, axis=0)
    return preprocess_input(img_array_expanded_dims)

# Select 3 random images from each class and predict
for class_name in class_names:
    folder_path = os.path.join(base_path, '/content/dataset', class_name)
    images = os.listdir(folder_path)
    selected_images = random.sample(images, 3)

    for img_name in selected_images:
        img_path = os.path.join(folder_path, img_name)

        # Print the path of the current image
        print(f"Image path: {img_path}")

        prepared_img = prepare_image(img_path)

        # Predict the class
        prediction = model_loaded.predict(prepared_img)
        predicted_class = class_names[np.argmax(prediction)]

        # Display the image and prediction
        img = plt.imread(img_path)
        plt.imshow(img)
        plt.title(f"True class: {class_name.capitalize()} \nPredicted class: {predicted_class.capitalize()}")
        plt.axis('off')
        plt.show()